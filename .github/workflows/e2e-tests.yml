name: End-to-End Testing Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - e2e
        - performance
        - user_journeys
        - load_test

env:
  PYTHON_VERSION: '3.13'
  NODE_VERSION: '18'

jobs:
  # Backend Unit and Integration Tests
  backend-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: test_riskintel360
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio pytest-cov pytest-xdist
    
    - name: Set up test environment
      run: |
        export DATABASE_URL=postgresql://test_user:test_password@localhost:5432/test_riskintel360
        export REDIS_URL=redis://localhost:6379/0
        export ENVIRONMENT=testing
        export AWS_ACCESS_KEY_ID=test-key
        export AWS_SECRET_ACCESS_KEY=test-secret
        export AWS_DEFAULT_REGION=us-east-1
    
    - name: Run backend unit tests
      run: |
        pytest tests/ -v --tb=short --cov=riskintel360 --cov-report=xml --cov-fail-under=80 -x
      env:
        DATABASE_URL: postgresql://test_user:test_password@localhost:5432/test_riskintel360
        REDIS_URL: redis://localhost:6379/0
        ENVIRONMENT: testing
    
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: backend
        name: backend-coverage

  # Frontend Tests
  frontend-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json
    
    - name: Install frontend dependencies
      working-directory: frontend
      run: npm ci
    
    - name: Run frontend unit tests
      working-directory: frontend
      run: npm run test:coverage
    
    - name: Run TypeScript type checking
      working-directory: frontend
      run: npm run type-check
    
    - name: Run frontend linting
      working-directory: frontend
      run: npm run lint
    
    - name: Build frontend
      working-directory: frontend
      run: npm run build
    
    - name: Upload frontend coverage
      uses: codecov/codecov-action@v3
      with:
        file: ./frontend/coverage/lcov.info
        flags: frontend
        name: frontend-coverage

  # End-to-End Tests with Docker
  e2e-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    needs: [backend-tests, frontend-tests]
    if: github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'e2e' || github.event.inputs.test_suite == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Create test environment file
      run: |
        cat > .env.test << EOF
        ENVIRONMENT=testing
        DATABASE_URL=postgresql://test_user:test_password@test-postgres:5432/test_riskintel360
        REDIS_URL=redis://:test_redis_password@test-redis:6379/0
        AWS_ACCESS_KEY_ID=test-access-key
        AWS_SECRET_ACCESS_KEY=test-secret-key
        AWS_DEFAULT_REGION=us-east-1
        BEDROCK_MOCK_MODE=true
        EXTERNAL_API_MOCK_MODE=true
        EOF
    
    - name: Build test images
      run: |
        docker-compose -f docker-compose.test.yml build
    
    - name: Start test services
      run: |
        docker-compose -f docker-compose.test.yml up -d test-postgres test-redis test-api
        
        # Wait for services to be ready
        echo "Waiting for services to be ready..."
        sleep 30
        
        # Health check
        docker-compose -f docker-compose.test.yml exec -T test-api curl -f http://localhost:8000/health || exit 1
    
    - name: Run end-to-end tests
      run: |
        docker-compose -f docker-compose.test.yml run --rm e2e-tests
      timeout-minutes: 45
    
    - name: Collect test results
      if: always()
      run: |
        mkdir -p test-results
        docker-compose -f docker-compose.test.yml logs test-api > test-results/api-logs.txt
        docker-compose -f docker-compose.test.yml logs e2e-tests > test-results/e2e-logs.txt
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-results
        path: test-results/
    
    - name: Cleanup test environment
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down --volumes --remove-orphans

  # Performance Tests
  performance-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [backend-tests]
    if: github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'performance' || github.event.inputs.test_suite == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Start test services for performance testing
      run: |
        docker-compose -f docker-compose.test.yml up -d test-postgres test-redis test-api
        
        # Wait for services
        sleep 30
        
        # Verify API is ready
        docker-compose -f docker-compose.test.yml exec -T test-api curl -f http://localhost:8000/health
    
    - name: Run performance tests
      run: |
        docker-compose -f docker-compose.test.yml run --rm -e TEST_SUITE=performance e2e-tests python -m pytest tests/performance/ -v --tb=short --maxfail=3
      timeout-minutes: 30
    
    - name: Collect performance results
      if: always()
      run: |
        mkdir -p performance-results
        docker-compose -f docker-compose.test.yml logs test-api > performance-results/api-performance-logs.txt
    
    - name: Upload performance results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-test-results
        path: performance-results/
    
    - name: Cleanup
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down --volumes

  # Load Tests
  load-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [backend-tests]
    if: github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'load_test'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Start services for load testing
      run: |
        docker-compose -f docker-compose.test.yml up -d test-postgres test-redis test-api
        sleep 30
    
    - name: Run load tests
      run: |
        docker-compose -f docker-compose.test.yml --profile load-test run --rm load-tests
      timeout-minutes: 20
    
    - name: Collect load test results
      if: always()
      run: |
        mkdir -p load-test-results
        docker cp $(docker-compose -f docker-compose.test.yml ps -q load-tests):/app/test-results/. load-test-results/ || true
    
    - name: Upload load test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: load-test-results/
    
    - name: Cleanup
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down --volumes

  # User Journey Tests
  user-journey-tests:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    needs: [backend-tests, frontend-tests]
    if: github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'user_journeys' || github.event.inputs.test_suite == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Start full test environment
      run: |
        docker-compose -f docker-compose.test.yml up -d test-postgres test-redis test-api test-frontend
        
        # Wait for all services
        sleep 45
        
        # Health checks
        docker-compose -f docker-compose.test.yml exec -T test-api curl -f http://localhost:8000/health
        curl -f http://localhost:3001 || echo "Frontend not ready, continuing..."
    
    - name: Run user journey tests
      run: |
        docker-compose -f docker-compose.test.yml run --rm e2e-tests python -m pytest tests/user_journeys/ -v --tb=short --maxfail=2
      timeout-minutes: 35
    
    - name: Collect user journey results
      if: always()
      run: |
        mkdir -p user-journey-results
        docker-compose -f docker-compose.test.yml logs test-api > user-journey-results/api-logs.txt
        docker-compose -f docker-compose.test.yml logs test-frontend > user-journey-results/frontend-logs.txt
    
    - name: Upload user journey results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: user-journey-results
        path: user-journey-results/
    
    - name: Cleanup
      if: always()
      run: |
        docker-compose -f docker-compose.test.yml down --volumes

  # Test Results Summary
  test-summary:
    runs-on: ubuntu-latest
    needs: [e2e-tests, performance-tests, user-journey-tests]
    if: always()
    
    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3
    
    - name: Generate test summary
      run: |
        echo "# RiskIntel360 Platform Test Results Summary" > test-summary.md
        echo "" >> test-summary.md
        echo "## Test Execution Summary" >> test-summary.md
        echo "- **Date**: $(date)" >> test-summary.md
        echo "- **Commit**: ${{ github.sha }}" >> test-summary.md
        echo "- **Branch**: ${{ github.ref_name }}" >> test-summary.md
        echo "" >> test-summary.md
        
        # Check job results
        echo "## Job Results" >> test-summary.md
        echo "- **Backend Tests**: ${{ needs.backend-tests.result || 'skipped' }}" >> test-summary.md
        echo "- **Frontend Tests**: ${{ needs.frontend-tests.result || 'skipped' }}" >> test-summary.md
        echo "- **E2E Tests**: ${{ needs.e2e-tests.result || 'skipped' }}" >> test-summary.md
        echo "- **Performance Tests**: ${{ needs.performance-tests.result || 'skipped' }}" >> test-summary.md
        echo "- **User Journey Tests**: ${{ needs.user-journey-tests.result || 'skipped' }}" >> test-summary.md
        echo "" >> test-summary.md
        
        # Overall status
        if [[ "${{ needs.e2e-tests.result }}" == "success" && "${{ needs.performance-tests.result }}" == "success" && "${{ needs.user-journey-tests.result }}" == "success" ]]; then
          echo "## ??Overall Status: PASSED" >> test-summary.md
          echo "All critical test suites passed successfully." >> test-summary.md
        else
          echo "## ??Overall Status: FAILED" >> test-summary.md
          echo "One or more critical test suites failed. Please review the results." >> test-summary.md
        fi
        
        echo "" >> test-summary.md
        echo "## Test Artifacts" >> test-summary.md
        echo "Test results and logs are available in the workflow artifacts." >> test-summary.md
        
        cat test-summary.md
    
    - name: Upload test summary
      uses: actions/upload-artifact@v3
      with:
        name: test-summary
        path: test-summary.md

  # Notification (optional)
  notify:
    runs-on: ubuntu-latest
    needs: [test-summary]
    if: always() && github.ref == 'refs/heads/main'
    
    steps:
    - name: Notify on failure
      if: needs.test-summary.result == 'failure'
      run: |
        echo "??E2E tests failed on main branch"
        echo "Please check the test results and fix any issues"
        # Add notification logic here (Slack, email, etc.)
    
    - name: Notify on success
      if: needs.test-summary.result == 'success'
      run: |
        echo "??All E2E tests passed on main branch"
        echo "RiskIntel360 platform is ready for deployment"
