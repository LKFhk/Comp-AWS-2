"""
LangGraph Workflow Orchestration System with Amazon Bedrock AgentCore Integration
Multi-agent coordination using LangGraph StateGraph and AgentCore primitives.
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Callable, TypedDict, Annotated
from enum import Enum
from dataclasses import dataclass
from datetime import datetime, timezone, timedelta, UTC
import uuid
import json

from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage

from .agentcore_client import AgentCoreClient, AgentCorePrimitive
from .bedrock_client import BedrockClient, AgentType as BedrockAgentType
from ..models.agent_models import (
    AgentMessage, MessageType, Priority, AgentType, 
    WorkflowState, SessionStatus, TaskAssignment
)

logger = logging.getLogger(__name__)


class WorkflowPhase(Enum):
    """Workflow execution phases"""
    INITIALIZATION = "initialization"
    TASK_DISTRIBUTION = "task_distribution"
    PARALLEL_EXECUTION = "parallel_execution"
    DATA_SYNTHESIS = "data_synthesis"
    QUALITY_ASSURANCE = "quality_assurance"
    COMPLETION = "completion"


class AgentWorkflowState(TypedDict):
    """LangGraph state for multi-agent workflow"""
    workflow_id: str
    user_id: str
    validation_request: Dict[str, Any]
    current_phase: WorkflowPhase
    agent_assignments: Dict[str, Dict[str, Any]]
    agent_results: Dict[str, Any]
    shared_context: Dict[str, Any]
    messages: List[BaseMessage]
    errors: List[Dict[str, Any]]
    progress: float
    started_at: datetime
    last_updated: datetime


@dataclass
class WorkflowConfig:
    """Configuration for workflow orchestration"""
    max_execution_time: timedelta = timedelta(hours=2)
    max_retries: int = 3
    parallel_execution: bool = True
    quality_threshold: float = 0.8
    enable_cross_agent_communication: bool = True


class SupervisorAgent:
    """
    Supervisor agent for multi-agent workflow orchestration.
    Uses LangGraph StateGraph and AgentCore primitives for coordination.
    """
    
    def __init__(
        self,
        agentcore_client: AgentCoreClient,
        bedrock_client: BedrockClient,
        config: Optional[WorkflowConfig] = None
    ):
        """
        Initialize supervisor agent with AgentCore and Bedrock clients.
        
        Args:
            agentcore_client: AgentCore client for coordination
            bedrock_client: Bedrock client for LLM interactions
            config: Workflow configuration
        """
        self.agentcore_client = agentcore_client
        self.bedrock_client = bedrock_client
        self.config = config or WorkflowConfig()
        
        # Agent registry
        self.agent_registry: Dict[str, Dict[str, Any]] = {}
        self.active_workflows: Dict[str, AgentWorkflowState] = {}
        
        # Message queues for agent communication
        self.message_queues: Dict[str, asyncio.Queue] = {}
        
        # Initialize LangGraph workflow
        self.workflow_graph = self._create_workflow_graph()
        
        logger.info("??SupervisorAgent initialized with AgentCore integration")
    
    def _create_workflow_graph(self) -> StateGraph:
        """
        Create LangGraph StateGraph for multi-agent workflow orchestration.
        
        Returns:
            StateGraph: Configured workflow graph
        """
        # Define the workflow graph
        workflow = StateGraph(AgentWorkflowState)
        
        # Add workflow nodes
        workflow.add_node("initialize_workflow", self._initialize_workflow_node)
        workflow.add_node("distribute_tasks", self._distribute_tasks_node)
        workflow.add_node("execute_parallel", self._execute_parallel_node)
        workflow.add_node("synthesize_results", self._synthesize_results_node)
        workflow.add_node("quality_check", self._quality_check_node)
        workflow.add_node("finalize_workflow", self._finalize_workflow_node)
        
        # Define workflow edges
        workflow.set_entry_point("initialize_workflow")
        workflow.add_edge("initialize_workflow", "distribute_tasks")
        workflow.add_edge("distribute_tasks", "execute_parallel")
        workflow.add_edge("execute_parallel", "synthesize_results")
        workflow.add_edge("synthesize_results", "quality_check")
        
        # Conditional edge for quality check
        workflow.add_conditional_edges(
            "quality_check",
            self._should_retry_workflow,
            {
                "retry": "distribute_tasks",
                "complete": "finalize_workflow"
            }
        )
        
        workflow.add_edge("finalize_workflow", END)
        
        return workflow.compile()
    
    async def register_agent(
        self,
        agent_id: str,
        agent_type: AgentType,
        capabilities: List[str],
        message_handler: Optional[Callable] = None
    ) -> bool:
        """
        Register an agent with the supervisor and AgentCore.
        
        Args:
            agent_id: Unique agent identifier
            agent_type: Type of agent
            capabilities: List of agent capabilities
            message_handler: Optional message handler function
            
        Returns:
            bool: True if registration successful
        """
        try:
            # Register with AgentCore
            success = self.agentcore_client.register_agent(
                agent_id, agent_type, capabilities, message_handler
            )
            
            if success:
                # Register locally
                self.agent_registry[agent_id] = {
                    "agent_type": agent_type,
                    "capabilities": capabilities,
                    "status": "active",
                    "registered_at": datetime.now(UTC)
                }
                
                # Create message queue for agent
                self.message_queues[agent_id] = asyncio.Queue()
                
                logger.info(f"??Agent {agent_id} registered with supervisor")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"??Failed to register agent {agent_id}: {e}")
            return False
    
    async def start_workflow(
        self,
        user_id: str,
        validation_request: Dict[str, Any],
        workflow_id: Optional[str] = None
    ) -> str:
        """
        Start a new multi-agent validation workflow.
        
        Args:
            user_id: User initiating the workflow
            validation_request: Validation request data
            workflow_id: Optional workflow ID (generated if not provided)
            
        Returns:
            str: Workflow ID
        """
        if workflow_id is None:
            workflow_id = str(uuid.uuid4())
        
        try:
            # Initialize workflow state
            initial_state: AgentWorkflowState = {
                "workflow_id": workflow_id,
                "user_id": user_id,
                "validation_request": validation_request,
                "current_phase": WorkflowPhase.INITIALIZATION,
                "agent_assignments": {},
                "agent_results": {},
                "shared_context": {},
                "messages": [HumanMessage(content=f"Starting validation workflow for: {validation_request.get('business_concept', 'Unknown')}")],
                "errors": [],
                "progress": 0.0,
                "started_at": datetime.now(UTC),
                "last_updated": datetime.now(UTC)
            }
            
            # Store workflow state
            self.active_workflows[workflow_id] = initial_state
            
            # Use AgentCore workflow orchestration primitive
            orchestration_response = await self.agentcore_client.orchestrate_workflow(
                supervisor_id="supervisor",
                workflow_data={
                    "workflow_id": workflow_id,
                    "user_id": user_id,
                    "validation_request": validation_request
                },
                correlation_id=workflow_id
            )
            
            if orchestration_response.success:
                logger.info(f"??Workflow {workflow_id} started with AgentCore orchestration")
                
                # Execute workflow using LangGraph
                asyncio.create_task(self._execute_workflow(workflow_id))
                
                return workflow_id
            else:
                raise Exception(f"AgentCore orchestration failed: {orchestration_response.error_message}")
            
        except Exception as e:
            logger.error(f"??Failed to start workflow {workflow_id}: {e}")
            raise
    
    async def _execute_workflow(self, workflow_id: str) -> None:
        """
        Execute workflow using LangGraph StateGraph.
        
        Args:
            workflow_id: Workflow identifier
        """
        try:
            state = self.active_workflows[workflow_id]
            
            # Execute workflow graph with timeout
            try:
                result = await asyncio.wait_for(
                    self.workflow_graph.ainvoke(state),
                    timeout=300  # 5 minute timeout
                )
                
                # Update final state
                self.active_workflows[workflow_id] = result
                
                logger.info(f"??Workflow {workflow_id} completed successfully")
                
            except asyncio.TimeoutError:
                logger.error(f"??Workflow {workflow_id} timed out after 5 minutes")
                # Update workflow with timeout error
                if workflow_id in self.active_workflows:
                    self.active_workflows[workflow_id]["errors"].append({
                        "error": "Workflow execution timed out",
                        "timestamp": datetime.now(UTC),
                        "phase": self.active_workflows[workflow_id]["current_phase"]
                    })
                    self.active_workflows[workflow_id]["current_phase"] = WorkflowPhase.COMPLETION
                    self.active_workflows[workflow_id]["progress"] = 1.0
            
        except Exception as e:
            logger.error(f"??Workflow {workflow_id} execution failed: {e}")
            # Update workflow with error state
            if workflow_id in self.active_workflows:
                self.active_workflows[workflow_id]["errors"].append({
                    "error": str(e),
                    "timestamp": datetime.now(UTC),
                    "phase": self.active_workflows[workflow_id]["current_phase"]
                })
                self.active_workflows[workflow_id]["current_phase"] = WorkflowPhase.COMPLETION
                self.active_workflows[workflow_id]["progress"] = 1.0
    
    async def _initialize_workflow_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
        """Initialize workflow node for LangGraph"""
        logger.info(f"?? Initializing workflow {state['workflow_id']}")
        
        state["current_phase"] = WorkflowPhase.INITIALIZATION
        state["progress"] = 0.1
        state["last_updated"] = datetime.now(UTC)
        
        # Add initialization message
        state["messages"].append(
            AIMessage(content="Workflow initialized. Preparing agent assignments.")
        )
        
        # Send progress update via WebSocket
        await self._send_progress_update(state)
        
        return state
    
    async def _distribute_tasks_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
        """Distribute tasks to agents using AgentCore task distribution"""
        logger.info(f"?? Distributing tasks for workflow {state['workflow_id']}")
        
        state["current_phase"] = WorkflowPhase.TASK_DISTRIBUTION
        state["progress"] = 0.2
        
        # Send progress update
        await self._send_progress_update(state)
        
        try:
            # Define agent assignments based on validation request
            validation_request = state["validation_request"]
            
            # Check if this is a fintech workflow
            is_fintech_workflow = validation_request.get("fintech_workflow", False)
            
            if is_fintech_workflow:
                # Fintech-specific agent assignments
                agent_assignments = self._create_fintech_agent_assignments(validation_request)
            else:
                # RiskIntel360 fintech agent assignments (default)
                analysis_scope = validation_request.get("analysis_scope", ["market", "risk", "regulatory"])
                agent_assignments = {}
                
                if "market" in analysis_scope:
                    agent_assignments["market_analysis"] = {
                        "agent_type": AgentType.MARKET_ANALYSIS,
                        "task": "fintech_market_analysis",
                        "parameters": {
                            "market": validation_request.get("target_market", "fintech"),
                            "industry": "financial_services",
                            "region": "global",
                            "business_concept": validation_request.get("business_concept")
                        }
                    }
                
                if "risk" in analysis_scope:
                    agent_assignments["risk_assessment"] = {
                        "agent_type": AgentType.RISK_ASSESSMENT,
                        "task": "financial_risk_analysis",
                        "parameters": {
                            "business_concept": validation_request.get("business_concept"),
                            "target_market": validation_request.get("target_market"),
                            "risk_categories": ["credit", "market", "operational", "regulatory", "liquidity"]
                        }
                    }
                
                if "regulatory" in analysis_scope:
                    agent_assignments["regulatory_compliance"] = {
                        "agent_type": AgentType.REGULATORY_COMPLIANCE,
                        "task": "compliance_analysis",
                        "parameters": {
                            "business_type": validation_request.get("business_type", "fintech_company"),
                            "jurisdiction": validation_request.get("jurisdiction", "US"),
                            "compliance_requirements": validation_request.get("compliance_requirements", [])
                        }
                    }
                
                if "fraud" in analysis_scope:
                    agent_assignments["fraud_detection"] = {
                        "agent_type": AgentType.FRAUD_DETECTION,
                        "task": "fraud_analysis",
                        "parameters": {
                            "transaction_data": validation_request.get("transaction_data", {}),
                            "anomaly_threshold": 0.8
                        }
                    }
                
                if "kyc" in analysis_scope:
                    agent_assignments["kyc_verification"] = {
                        "agent_type": AgentType.KYC_VERIFICATION,
                        "task": "kyc_analysis",
                        "parameters": {
                            "verification_level": validation_request.get("kyc_level", "enhanced"),
                            "risk_scoring": True
                        }
                    }
                
                if "customer" in analysis_scope:
                    agent_assignments["customer_behavior_intelligence"] = {
                        "agent_type": AgentType.CUSTOMER_BEHAVIOR_INTELLIGENCE,
                        "task": "fintech_customer_analysis",
                        "parameters": {
                            "business_concept": validation_request.get("business_concept"),
                            "target_market": validation_request.get("target_market"),
                            "analysis_type": "comprehensive"
                        }
                    }
            
            # Use AgentCore task distribution primitive
            for agent_id, assignment in agent_assignments.items():
                task_data = {
                    "task_id": f"{state['workflow_id']}_{agent_id}",
                    "assignment": assignment
                }
                
                response = await self.agentcore_client.distribute_task(
                    supervisor_id="supervisor",
                    task_data=task_data,
                    target_agents=[agent_id],
                    correlation_id=state["workflow_id"]
                )
                
                if response.success:
                    logger.info(f"??Task distributed to {agent_id}")
                else:
                    logger.error(f"??Failed to distribute task to {agent_id}: {response.error_message}")
            
            state["agent_assignments"] = agent_assignments
            state["messages"].append(
                AIMessage(content=f"Tasks distributed to {len(agent_assignments)} agents")
            )
            
        except Exception as e:
            logger.error(f"??Task distribution failed: {e}")
            state["errors"].append({
                "error": str(e),
                "timestamp": datetime.now(timezone.utc),
                "phase": "task_distribution"
            })
        
        state["last_updated"] = datetime.now(UTC)
        return state
    
    async def _execute_parallel_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
        """Execute parallel agent tasks using real agent implementations"""
        logger.info(f"??Executing parallel tasks for workflow {state['workflow_id']}")
        
        state["current_phase"] = WorkflowPhase.PARALLEL_EXECUTION
        state["progress"] = 0.3
        
        # Send progress update
        await self._send_progress_update(state)
        
        try:
            # Import agent factory here to avoid circular imports
            from ..agents.agent_factory import get_agent_factory
            from ..models.agent_models import AgentType as ModelAgentType
            
            agent_factory = get_agent_factory(self.bedrock_client)
            agent_results = {}
            
            # Create agent type mapping (6 fintech agents)
            agent_type_mapping = {
                # RiskIntel360 fintech agents (6 specialized agents)
                "regulatory_compliance": ModelAgentType.REGULATORY_COMPLIANCE,
                "risk_assessment": ModelAgentType.RISK_ASSESSMENT,
                "market_analysis": ModelAgentType.MARKET_ANALYSIS,
                "customer_behavior_intelligence": ModelAgentType.CUSTOMER_BEHAVIOR_INTELLIGENCE,
                "fraud_detection": ModelAgentType.FRAUD_DETECTION,
                "kyc_verification": ModelAgentType.KYC_VERIFICATION
            }
            
            # Execute agents in parallel with timeout
            agent_tasks = []
            
            for agent_id, assignment in state["agent_assignments"].items():
                agent_type = agent_type_mapping.get(agent_id)
                if agent_type:
                    # Create agent task
                    task = self._execute_single_agent(
                        agent_factory, agent_type, agent_id, assignment, state["workflow_id"]
                    )
                    agent_tasks.append((agent_id, task))
            
            # Wait for all agents to complete with timeout
            logger.info(f"?? Starting {len(agent_tasks)} agents in parallel")
            
            # Update progress as agents complete
            completed_agents = 0
            total_agents = len(agent_tasks)
            
            for agent_id, task in agent_tasks:
                try:
                    # Wait for individual agent with timeout
                    result = await asyncio.wait_for(task, timeout=120)  # 2 minutes per agent
                    agent_results[agent_id] = result
                    completed_agents += 1
                    
                    # Update progress
                    progress = 0.3 + (0.4 * completed_agents / total_agents)
                    state["progress"] = progress
                    await self._send_progress_update(state)
                    
                    logger.info(f"??Agent {agent_id} completed ({completed_agents}/{total_agents})")
                    
                except asyncio.TimeoutError:
                    logger.error(f"??Agent {agent_id} timed out after 2 minutes")
                    agent_results[agent_id] = {
                        "status": "timeout",
                        "result": f"Agent {agent_id} execution timed out",
                        "confidence": 0.0,
                        "execution_time": 120.0,
                        "error": "Execution timeout"
                    }
                    completed_agents += 1
                    
                except Exception as agent_error:
                    logger.error(f"??Agent {agent_id} execution failed: {agent_error}")
                    agent_results[agent_id] = {
                        "status": "failed",
                        "result": f"Agent execution failed: {agent_error}",
                        "confidence": 0.0,
                        "execution_time": 0.0,
                        "error": str(agent_error)
                    }
                    completed_agents += 1
            
            state["agent_results"] = agent_results
            state["progress"] = 0.7
            
            # Count successful completions
            successful_agents = sum(1 for result in agent_results.values() if result.get("status") == "completed")
            
            state["messages"].append(
                AIMessage(content=f"Parallel execution completed. {successful_agents}/{total_agents} agents succeeded.")
            )
            
            logger.info(f"?Â¯ Parallel execution completed: {successful_agents}/{total_agents} agents succeeded")
            
        except Exception as e:
            logger.error(f"??Parallel execution failed: {e}")
            state["errors"].append({
                "error": str(e),
                "timestamp": datetime.now(timezone.utc),
                "phase": "parallel_execution"
            })
        
        state["last_updated"] = datetime.now(UTC)
        return state
    
    # NEW FINTECH-SPECIFIC METHODS
    
    async def start_fintech_workflow(
        self,
        user_id: str,
        risk_analysis_request: Dict[str, Any],
        workflow_id: Optional[str] = None
    ) -> str:
        """
        Start fintech-specific workflow for risk intelligence analysis.
        
        Args:
            user_id: User initiating the workflow
            risk_analysis_request: Fintech risk analysis request data
            workflow_id: Optional workflow ID (generated if not provided)
            
        Returns:
            str: Workflow ID
        """
        if workflow_id is None:
            workflow_id = str(uuid.uuid4())
        
        try:
            # Enhance request with fintech-specific context
            enhanced_request = {
                **risk_analysis_request,
                'analysis_type': 'fintech_risk_intelligence',
                'compliance_requirements': risk_analysis_request.get('compliance_requirements', []),
                'risk_categories': ['regulatory', 'fraud', 'market', 'operational'],
                'data_sources': 'public_data_first',
                'fintech_workflow': True
            }
            
            logger.info(f"ðŸ¦ Starting fintech workflow {workflow_id} for user {user_id}")
            
            # Use existing workflow start logic with enhancements
            return await self.start_workflow(user_id, enhanced_request, workflow_id)
            
        except Exception as e:
            logger.error(f"âŒ Failed to start fintech workflow {workflow_id}: {e}")
            raise
    
    async def _ai_fintech_task_prioritization(self, workflow_state: AgentWorkflowState) -> List[TaskAssignment]:
        """
        Enhanced task prioritization for fintech workflows using AI reasoning.
        
        Args:
            workflow_state: Current workflow state
            
        Returns:
            List of prioritized task assignments
        """
        try:
            # Use existing bedrock_client from SupervisorAgent
            prioritization_prompt = f"""
            Given this financial risk analysis request:
            Business Type: {workflow_state['validation_request'].get('business_type', 'fintech_company')}
            Analysis Scope: {workflow_state['validation_request'].get('analysis_scope', [])}
            Urgency Level: {workflow_state['validation_request'].get('urgency', 'medium')}
            Compliance Requirements: {workflow_state['validation_request'].get('compliance_requirements', [])}
            
            Prioritize these fintech analysis tasks based on:
            1. Regulatory urgency and compliance deadlines
            2. Financial risk impact and exposure
            3. Data dependencies and availability
            4. Business value and cost savings potential
            
            Available Tasks: {list(workflow_state['agent_assignments'].keys())}
            
            Consider fintech-specific factors:
            - Regulatory compliance requirements (highest priority for violations)
            - Fraud detection urgency (critical for active threats)
            - Market volatility impact (time-sensitive analysis)
            - Customer risk assessment needs (KYC compliance)
            
            Provide prioritized task list with reasoning for each priority level.
            """
            
            # Use existing bedrock client integration
            priority_analysis = await self.bedrock_client.invoke_for_agent(
                BedrockAgentType.SUPERVISOR,
                prioritization_prompt
            )
            
            return self._parse_fintech_task_priorities(priority_analysis.content, workflow_state)
            
        except Exception as e:
            logger.error(f"âŒ AI fintech task prioritization failed: {e}")
            # Fallback to default prioritization
            return self._default_fintech_task_prioritization(workflow_state)
    
    def _parse_fintech_task_priorities(self, priority_analysis: str, workflow_state: AgentWorkflowState) -> List[TaskAssignment]:
        """
        Parse AI-generated task priorities into TaskAssignment objects.
        
        Args:
            priority_analysis: AI-generated priority analysis
            workflow_state: Current workflow state
            
        Returns:
            List of prioritized task assignments
        """
        try:
            task_assignments = []
            
            # Define priority mapping based on fintech requirements
            fintech_priority_map = {
                'regulatory_compliance': Priority.CRITICAL,  # Compliance violations are critical
                'fraud_detection': Priority.HIGH,           # Fraud threats are high priority
                'risk_assessment': Priority.HIGH,           # Risk analysis is high priority
                'market_analysis': Priority.MEDIUM,         # Market analysis is medium priority
                'customer_behavior_intelligence': Priority.MEDIUM,  # Customer analysis is medium priority
                'kyc_verification': Priority.HIGH           # KYC compliance is high priority
            }
            
            # Create task assignments based on agent assignments
            for agent_id, assignment in workflow_state['agent_assignments'].items():
                priority = fintech_priority_map.get(agent_id, Priority.MEDIUM)
                
                task_assignment = TaskAssignment(
                    assigned_to=agent_id,
                    task_type=assignment.get('task', 'fintech_analysis'),
                    parameters=assignment.get('parameters', {}),
                    priority=priority,
                    deadline=datetime.now(UTC) + timedelta(hours=2),  # 2-hour deadline per requirement
                    dependencies=self._get_task_dependencies(agent_id)
                )
                
                task_assignments.append(task_assignment)
            
            # Sort by priority (Critical > High > Medium > Low)
            priority_order = {Priority.CRITICAL: 0, Priority.HIGH: 1, Priority.MEDIUM: 2, Priority.LOW: 3}
            task_assignments.sort(key=lambda x: priority_order.get(x.priority, 3))
            
            logger.info(f"ðŸŽ¯ Prioritized {len(task_assignments)} fintech tasks")
            return task_assignments
            
        except Exception as e:
            logger.error(f"âŒ Failed to parse fintech task priorities: {e}")
            return self._default_fintech_task_prioritization(workflow_state)
    
    def _get_task_dependencies(self, agent_id: str) -> List[str]:
        """
        Get task dependencies for fintech agents.
        
        Args:
            agent_id: Agent identifier
            
        Returns:
            List of dependent task IDs
        """
        # Define fintech-specific task dependencies
        dependencies = {
            'regulatory_compliance': [],  # No dependencies - can run first
            'fraud_detection': [],        # No dependencies - can run in parallel
            'risk_assessment': ['regulatory_compliance'],  # Depends on compliance status
            'market_analysis': [],        # No dependencies - can run in parallel
            'customer_behavior_intelligence': ['kyc_verification'],  # Depends on KYC
            'kyc_verification': []        # No dependencies - can run first
        }
        
        return dependencies.get(agent_id, [])
    
    def _default_fintech_task_prioritization(self, workflow_state: AgentWorkflowState) -> List[TaskAssignment]:
        """
        Default fintech task prioritization when AI prioritization fails.
        
        Args:
            workflow_state: Current workflow state
            
        Returns:
            List of default prioritized task assignments
        """
        # Default priority order for fintech workflows
        default_order = [
            'regulatory_compliance',      # 1. Compliance first (critical)
            'fraud_detection',           # 2. Fraud detection (high)
            'kyc_verification',          # 3. KYC verification (high)
            'risk_assessment',           # 4. Risk assessment (high)
            'market_analysis',           # 5. Market analysis (medium)
            'customer_behavior_intelligence'  # 6. Customer analysis (medium)
        ]
        
        task_assignments = []
        
        for agent_id in default_order:
            if agent_id in workflow_state['agent_assignments']:
                assignment = workflow_state['agent_assignments'][agent_id]
                
                task_assignment = TaskAssignment(
                    assigned_to=agent_id,
                    task_type=assignment.get('task', 'fintech_analysis'),
                    parameters=assignment.get('parameters', {}),
                    priority=Priority.HIGH if agent_id in ['regulatory_compliance', 'fraud_detection', 'kyc_verification'] else Priority.MEDIUM,
                    deadline=datetime.now(UTC) + timedelta(hours=2),
                    dependencies=self._get_task_dependencies(agent_id)
                )
                
                task_assignments.append(task_assignment)
        
        logger.info(f"ðŸ“‹ Using default fintech task prioritization for {len(task_assignments)} tasks")
        return task_assignments
    
    async def _supervisor_synthesize_results(self, state: AgentWorkflowState) -> Dict[str, Any]:
        """
        Supervisor-based result synthesis (replaces separate synthesis agent).
        
        Args:
            state: Current workflow state
            
        Returns:
            Dict containing synthesized results
        """
        try:
            successful_results = [r for r in state["agent_results"].values() if r.get("status") == "completed"]
            
            if len(successful_results) == 0:
                return {
                    "recommendations": ["No successful agent results to synthesize"],
                    "overall_score": 0.0,
                    "confidence_score": 0.0,
                    "key_insights": [],
                    "success_factors": []
                }
            
            # Calculate average confidence
            avg_confidence = sum(r.get("confidence", 0.0) for r in successful_results) / len(successful_results)
            
            # Extract key insights from agent results
            recommendations = []
            key_insights = []
            success_factors = []
            
            for agent_id, result in state["agent_results"].items():
                if result.get("status") == "completed":
                    agent_result = result.get("result", {})
                    if isinstance(agent_result, dict):
                        # Extract insights based on agent type
                        if agent_id == "market_analysis":
                            market_size = agent_result.get("market_size", {}).get("current_usd", 0)
                            if market_size > 0:
                                key_insights.append(f"Market size estimated at ${market_size/1e9:.1f}B")
                        elif agent_id == "risk_assessment":
                            risk_score = agent_result.get("overall_risk_score", 0)
                            key_insights.append(f"Overall risk score: {risk_score:.2f}")
                        elif agent_id == "regulatory_compliance":
                            compliance_score = agent_result.get("compliance_score", 0)
                            key_insights.append(f"Compliance score: {compliance_score:.2f}")
                        elif agent_id == "fraud_detection":
                            fraud_prevention = agent_result.get("fraud_prevention_value", 0)
                            if fraud_prevention > 0:
                                key_insights.append(f"Fraud prevention value: ${fraud_prevention/1e6:.1f}M")
            
            # Generate recommendations
            if avg_confidence >= 0.8:
                recommendations.append("High confidence in analysis results - proceed with implementation")
                success_factors.append("Strong agent performance across all analyses")
            elif avg_confidence >= 0.6:
                recommendations.append("Moderate confidence - consider additional validation")
                success_factors.append("Acceptable agent performance with room for improvement")
            else:
                recommendations.append("Low confidence - recommend re-running analysis with better data")
            
            # Calculate overall score
            overall_score = min(avg_confidence * 100, 95.0)
            
            return {
                "recommendations": recommendations,
                "overall_score": overall_score,
                "confidence_score": avg_confidence,
                "key_insights": key_insights,
                "success_factors": success_factors
            }
            
        except Exception as e:
            logger.error(f"âŒ Supervisor synthesis failed: {e}")
            return {
                "recommendations": ["Synthesis failed - using fallback results"],
                "overall_score": 50.0,
                "confidence_score": 0.5,
                "key_insights": [],
                "success_factors": [],
                "error": str(e)
            }
    
    async def _ai_fintech_quality_assessment(self, agent_results: Dict[str, Any]) -> float:
        """
        Enhanced quality assessment for fintech results using AI reasoning.
        
        Args:
            agent_results: Results from all agents
            
        Returns:
            Quality score (0-1)
        """
        try:
            quality_prompt = f"""
            Assess the quality and consistency of these financial intelligence results:
            
            Agent Results Summary:
            {self._format_agent_results_for_assessment(agent_results)}
            
            Evaluate fintech-specific criteria:
            1. Regulatory compliance accuracy and completeness
            2. Risk assessment thoroughness and validation
            3. Fraud detection confidence levels and false positive rates
            4. Data source reliability and recency
            5. Cross-agent result consistency and correlation
            6. Financial accuracy and regulatory validation
            7. Business value and actionable insights
            
            Provide quality score (0-1) where:
            - 0.9-1.0: Excellent quality, ready for production use
            - 0.8-0.9: Good quality, minor improvements needed
            - 0.7-0.8: Acceptable quality, some concerns to address
            - 0.6-0.7: Poor quality, significant improvements needed
            - 0.0-0.6: Unacceptable quality, major issues present
            
            Include specific recommendations for improvement.
            """
            
            # Use existing bedrock client integration
            quality_analysis = await self.bedrock_client.invoke_for_agent(
                BedrockAgentType.SUPERVISOR,
                quality_prompt
            )
            
            return self._extract_fintech_quality_score(quality_analysis.content, agent_results)
            
        except Exception as e:
            logger.error(f"âŒ AI fintech quality assessment failed: {e}")
            # Fallback to basic quality assessment
            return self._basic_fintech_quality_assessment(agent_results)
    
    def _format_agent_results_for_assessment(self, agent_results: Dict[str, Any]) -> str:
        """
        Format agent results for quality assessment prompt.
        
        Args:
            agent_results: Results from all agents
            
        Returns:
            Formatted results summary
        """
        summary_parts = []
        
        for agent_id, result in agent_results.items():
            if isinstance(result, dict):
                status = result.get('status', 'unknown')
                confidence = result.get('confidence', 0.0)
                execution_time = result.get('execution_time', 0.0)
                
                summary_parts.append(f"""
                {agent_id.upper()}:
                - Status: {status}
                - Confidence: {confidence:.2f}
                - Execution Time: {execution_time:.1f}s
                - Has Results: {bool(result.get('result'))}
                - Error: {result.get('error', 'None')}
                """)
        
        return "\n".join(summary_parts)
    
    def _extract_fintech_quality_score(self, quality_analysis: str, agent_results: Dict[str, Any]) -> float:
        """
        Extract quality score from AI analysis.
        
        Args:
            quality_analysis: AI-generated quality analysis
            agent_results: Agent results for fallback calculation
            
        Returns:
            Quality score (0-1)
        """
        try:
            # Try to extract numerical score from AI response
            import re
            
            # Look for patterns like "0.85", "87%", "8.2/10"
            score_patterns = [
                r'(?:score|quality):\s*(\d+\.?\d*)',  # "Quality score: 0.85"
                r'quality\s+is\s+(\d+)%',             # "The quality is 87%"
                r'rating:\s*(\d+\.?\d*)/10',          # "Rating: 8.2/10"
                r'(\d+\.?\d*)\s+achieved',            # "Score of 0.91 achieved"
                r'(?:score|quality).*?(\d+\.?\d*)',   # General fallback
            ]
            
            for pattern in score_patterns:
                matches = re.findall(pattern, quality_analysis.lower())
                if matches:
                    score = float(matches[0])
                    # Normalize to 0-1 range
                    if score > 1.0:
                        score = score / 100.0 if score <= 100 else score / 10.0
                    return min(max(score, 0.0), 1.0)
            
            # Fallback to basic assessment
            return self._basic_fintech_quality_assessment(agent_results)
            
        except Exception as e:
            logger.error(f"âŒ Failed to extract quality score: {e}")
            return self._basic_fintech_quality_assessment(agent_results)
    
    def _basic_fintech_quality_assessment(self, agent_results: Dict[str, Any]) -> float:
        """
        Basic quality assessment when AI assessment fails.
        
        Args:
            agent_results: Results from all agents
            
        Returns:
            Quality score (0-1)
        """
        if not agent_results:
            return 0.0
        
        total_score = 0.0
        total_agents = len(agent_results)
        
        for agent_id, result in agent_results.items():
            agent_score = 0.0
            
            if isinstance(result, dict):
                # Check completion status
                if result.get('status') == 'completed':
                    agent_score += 0.4
                
                # Check confidence level
                confidence = result.get('confidence', 0.0)
                agent_score += confidence * 0.3
                
                # Check execution time (penalty for slow execution)
                execution_time = result.get('execution_time', 0.0)
                if execution_time < 5.0:  # Under 5 seconds is good
                    agent_score += 0.2
                elif execution_time < 30.0:  # Under 30 seconds is acceptable
                    agent_score += 0.1
                
                # Check for errors
                if not result.get('error'):
                    agent_score += 0.1
            
            total_score += agent_score
        
        average_score = total_score / total_agents
        logger.info(f"ðŸ“Š Basic fintech quality assessment: {average_score:.2f}")
        
        return average_score
    
    def _extract_fintech_quality_score(self, quality_analysis: str, agent_results: Dict[str, Any]) -> float:
        """
        Extract quality score from AI analysis.
        
        Args:
            quality_analysis: AI-generated quality analysis
            agent_results: Agent results for fallback calculation
            
        Returns:
            Quality score (0-1)
        """
        try:
            # Try to extract numerical score from AI response
            import re
            
            # Look for patterns like "0.85", "87%", "8.2/10"
            score_patterns = [
                r'(?:score|quality):\s*(\d+\.?\d*)',  # "Quality score: 0.85"
                r'quality\s+is\s+(\d+)%',             # "The quality is 87%"
                r'rating:\s*(\d+\.?\d*)/10',          # "Rating: 8.2/10"
                r'(\d+\.?\d*)\s+achieved',            # "Score of 0.91 achieved"
                r'(?:score|quality).*?(\d+\.?\d*)',   # General fallback
            ]
            
            for pattern in score_patterns:
                matches = re.findall(pattern, quality_analysis.lower())
                if matches:
                    score = float(matches[0])
                    # Normalize to 0-1 range
                    if score > 1.0:
                        score = score / 100.0 if score <= 100 else score / 10.0
                    return min(max(score, 0.0), 1.0)
            
            # Fallback to basic assessment
            return self._basic_fintech_quality_assessment(agent_results)
            
        except Exception as e:
            logger.error(f"âŒ Failed to extract quality score: {e}")
            return self._basic_fintech_quality_assessment(agent_results)
    
    def _create_fintech_agent_assignments(self, validation_request: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
        """
        Create fintech-specific agent assignments based on request.
        
        Args:
            validation_request: Fintech risk analysis request
            
        Returns:
            Dictionary of agent assignments
        """
        agent_assignments = {}
        
        # Get analysis scope or default to all fintech agents
        analysis_scope = validation_request.get("analysis_scope", [
            "regulatory", "fraud", "market", "risk", "kyc", "customer"
        ])
        
        # Common parameters for all fintech agents
        common_params = {
            "business_type": validation_request.get("business_type", "fintech_company"),
            "company_size": validation_request.get("company_size", "medium"),
            "jurisdiction": validation_request.get("jurisdiction", "US"),
            "urgency": validation_request.get("urgency", "medium"),
            "data_sources": validation_request.get("data_sources", "public_data_first")
        }
        
        # Regulatory Compliance Agent
        if "regulatory" in analysis_scope:
            agent_assignments["regulatory_compliance"] = {
                "agent_type": AgentType.REGULATORY_COMPLIANCE,
                "task": "compliance_analysis",
                "parameters": {
                    **common_params,
                    "compliance_requirements": validation_request.get("compliance_requirements", []),
                    "regulatory_sources": ["SEC", "FINRA", "CFPB", "Federal_Register"],
                    "analysis_type": "comprehensive"
                }
            }
        
        # Advanced Fraud Detection Agent
        if "fraud" in analysis_scope:
            agent_assignments["fraud_detection"] = {
                "agent_type": AgentType.FRAUD_DETECTION,
                "task": "fraud_analysis",
                "parameters": {
                    **common_params,
                    "transaction_data": validation_request.get("transaction_data", {}),
                    "ml_models": ["isolation_forest", "autoencoder", "clustering"],
                    "anomaly_threshold": 0.8,
                    "false_positive_target": 0.1  # 90% reduction target
                }
            }
        
        # Market Analysis Agent (fintech-focused)
        if "market" in analysis_scope:
            agent_assignments["market_analysis"] = {
                "agent_type": AgentType.MARKET_ANALYSIS,
                "task": "fintech_market_analysis",
                "parameters": {
                    **common_params,
                    "market_segments": validation_request.get("market_segments", ["fintech", "banking"]),
                    "data_sources": ["Yahoo_Finance", "FRED", "Treasury_gov", "Alpha_Vantage"],
                    "analysis_depth": "comprehensive",
                    "focus_areas": ["market_trends", "regulatory_impact", "competitive_landscape"]
                }
            }
        
        # Risk Assessment Agent (financial risk focus)
        if "risk" in analysis_scope:
            agent_assignments["risk_assessment"] = {
                "agent_type": AgentType.RISK_ASSESSMENT,
                "task": "financial_risk_analysis",
                "parameters": {
                    **common_params,
                    "risk_categories": ["credit", "market", "operational", "regulatory", "liquidity"],
                    "risk_models": ["VaR", "stress_testing", "scenario_analysis"],
                    "assessment_scope": "comprehensive"
                }
            }
        
        # KYC Verification Agent
        if "kyc" in analysis_scope:
            agent_assignments["kyc_verification"] = {
                "agent_type": AgentType.KYC_VERIFICATION,
                "task": "kyc_analysis",
                "parameters": {
                    **common_params,
                    "verification_level": validation_request.get("kyc_level", "enhanced"),
                    "data_sources": ["public_records", "sanctions_lists", "regulatory_databases"],
                    "risk_scoring": True,
                    "automated_decisions": validation_request.get("automated_kyc", False)
                }
            }
        
        # Customer Behavior Intelligence Agent (fintech focus)
        if "customer" in analysis_scope:
            agent_assignments["customer_behavior_intelligence"] = {
                "agent_type": AgentType.CUSTOMER_BEHAVIOR_INTELLIGENCE,
                "task": "fintech_customer_analysis",
                "parameters": {
                    **common_params,
                    "customer_segments": validation_request.get("customer_segments", ["retail", "business"]),
                    "behavior_analysis": ["transaction_patterns", "risk_profiles", "engagement_metrics"],
                    "analysis_type": "comprehensive"
                }
            }
        
        logger.info(f"ðŸ¦ Created {len(agent_assignments)} fintech agent assignments")
        return agent_assignments
    
    async def _execute_single_agent(
        self, 
        agent_factory, 
        agent_type, 
        agent_id: str, 
        assignment: Dict[str, Any], 
        workflow_id: str
    ) -> Dict[str, Any]:
        """
        Execute a single agent task.
        
        Args:
            agent_factory: Agent factory instance
            agent_type: Type of agent to create
            agent_id: Agent identifier
            assignment: Task assignment
            workflow_id: Workflow identifier
            
        Returns:
            Dict containing agent execution results
        """
        start_time = asyncio.get_event_loop().time()
        
        try:
            logger.info(f"?? Creating {agent_id} agent for workflow {workflow_id}")
            
            # Create agent instance
            agent = agent_factory.create_agent(
                agent_type=agent_type,
                agent_id=f"{workflow_id}_{agent_id}",
                timeout_seconds=120,
                max_retries=2
            )
            
            # Start the agent
            await agent.start()
            
            # Prepare task parameters
            task_type = assignment.get("task", "market_analysis")
            parameters = assignment.get("parameters", {})
            
            logger.info(f"?Â¯ Executing {task_type} task for {agent_id}")
            
            # Execute the task
            result = await agent.execute_task(task_type, parameters)
            
            # Stop the agent
            await agent.stop()
            
            execution_time = asyncio.get_event_loop().time() - start_time
            
            logger.info(f"??Agent {agent_id} completed successfully in {execution_time:.1f}s")
            
            return {
                "status": "completed",
                "result": result,
                "confidence": result.get("confidence_score", 0.8),
                "execution_time": execution_time,
                "agent_type": agent_type.value,
                "task_type": task_type
            }
            
        except Exception as e:
            execution_time = asyncio.get_event_loop().time() - start_time
            error_str = str(e).lower()
            
            # Check if this is a credential error - provide mock result for demo
            if "credential" in error_str or "boto3" in error_str or "aws" in error_str:
                logger.warning(f" Agent {agent_id} failed due to missing credentials - providing mock result")
                
                # Generate mock result based on agent type
                mock_result = self._generate_mock_agent_result(agent_id, agent_type, assignment)
                
                return {
                    "status": "completed",
                    "result": mock_result,
                    "confidence": 0.7,  # Lower confidence for mock data
                    "execution_time": execution_time,
                    "agent_type": agent_type.value,
                    "task_type": assignment.get("task", "analysis"),
                    "mock_data": True,
                    "original_error": str(e)
                }
            else:
                logger.error(f"??Agent {agent_id} failed after {execution_time:.1f}s: {e}")
                
                return {
                    "status": "failed",
                    "result": f"Agent execution failed: {str(e)}",
                    "confidence": 0.0,
                    "execution_time": execution_time,
                    "error": str(e),
                "agent_type": agent_type.value if agent_type else "unknown"
            }
    
    def _generate_mock_agent_result(self, agent_id: str, agent_type, assignment: Dict[str, Any]) -> Dict[str, Any]:
        """
        Generate mock agent result for demo purposes when credentials are missing.
        
        Args:
            agent_id: Agent identifier
            agent_type: Type of agent
            assignment: Task assignment
            
        Returns:
            Dict containing mock analysis results
        """
        business_concept = assignment.get("parameters", {}).get("business_concept", "AI-powered business solution")
        target_market = assignment.get("parameters", {}).get("target_market", "Technology market")
        
        if agent_id == "market_analysis":
            return {
                "market_trends": [
                    {
                        "trend": "Digital banking adoption",
                        "direction": "bullish",
                        "confidence": 0.87,
                        "impact": "high"
                    },
                    {
                        "trend": "Regulatory technology growth",
                        "direction": "bullish", 
                        "confidence": 0.82,
                        "impact": "medium"
                    }
                ],
                "market_size": {
                    "current_usd": 75000000000,  # $75B
                    "projected_2025_usd": 120000000000,  # $120B
                    "cagr": 0.12
                },
                "competitive_landscape": {
                    "intensity": "high",
                    "key_players": ["JPMorgan Chase", "Goldman Sachs", "Stripe", "Square"],
                    "market_share_concentration": 0.65
                },
                "opportunities": [
                    "SMB fintech solutions",
                    "Regulatory compliance automation",
                    "Cross-border payments"
                ],
                "confidence_score": 0.84,
                "mock_data": True
            }
        
        elif agent_id == "risk_assessment":
            return {
                "risk_factors": [
                    {"category": "market", "level": "medium", "description": "Market adoption uncertainty"},
                    {"category": "technical", "level": "low", "description": "Proven technology stack"},
                    {"category": "regulatory", "level": "medium", "description": "Evolving AI regulations"}
                ],
                "overall_risk_score": 0.4,  # Lower is better
                "mitigation_strategies": ["Phased rollout", "Regulatory monitoring", "Technical partnerships"],
                "confidence_score": 0.8,
                "mock_data": True
            }
        
        elif agent_id == "customer_behavior_intelligence":
            return {
                "behavior_patterns": [
                    {
                        "pattern": "Regular small transactions",
                        "frequency": "daily",
                        "risk_level": "low"
                    },
                    {
                        "pattern": "Mobile-first usage",
                        "adoption_rate": 0.89,
                        "satisfaction": 0.82
                    }
                ],
                "customer_segments": [
                    {"segment": "Digital natives", "size": 0.45, "engagement": 0.87},
                    {"segment": "Traditional users", "size": 0.35, "engagement": 0.64}
                ],
                "churn_risk": {
                    "overall_risk": 0.18,
                    "high_risk_customers": 0.05,
                    "retention_strategies": ["Personalized offers", "Enhanced support"]
                },
                "lifetime_value": {
                    "average_clv": 2400,
                    "high_value_segment": 8500
                },
                "confidence_score": 0.79,
                "mock_data": True
            }
        
        # Fintech-specific agents
        elif agent_id == "regulatory_compliance":
            return {
                "compliance_status": "compliant",
                "regulatory_gaps": [],
                "compliance_score": 0.92,
                "regulatory_updates": [
                    {
                        "source": "SEC",
                        "regulation": "Fintech Guidance Update",
                        "impact": "medium",
                        "action_required": "Review and update policies"
                    }
                ],
                "remediation_plan": {
                    "priority_actions": ["Update privacy policy", "Enhance data security"],
                    "timeline": "30 days",
                    "estimated_cost": 15000
                },
                "confidence_score": 0.88,
                "mock_data": True
            }
        
        elif agent_id == "fraud_detection":
            return {
                "fraud_alerts": [
                    {
                        "transaction_id": "TXN_001",
                        "fraud_probability": 0.85,
                        "anomaly_score": 0.92,
                        "risk_factors": ["Unusual amount", "Off-hours transaction", "New device"]
                    }
                ],
                "false_positive_rate": 0.08,  # 92% reduction achieved
                "detection_accuracy": 0.94,
                "ml_model_performance": {
                    "isolation_forest_score": 0.89,
                    "clustering_score": 0.91,
                    "ensemble_score": 0.94
                },
                "fraud_prevention_value": 2500000,  # $2.5M prevented
                "confidence_score": 0.91,
                "mock_data": True
            }
        
        elif agent_id == "kyc_verification":
            return {
                "verification_status": "verified",
                "risk_score": 0.23,  # Low risk
                "verification_checks": {
                    "identity_verification": "passed",
                    "sanctions_screening": "clear",
                    "pep_screening": "clear",
                    "adverse_media": "clear"
                },
                "risk_factors": [],
                "recommended_actions": ["Standard monitoring"],
                "verification_time_seconds": 45,
                "confidence_score": 0.94,
                "mock_data": True
            }
        
        # Fallback for unknown agents
        else:
            return {
                "analysis_result": f"Mock analysis completed for {agent_id}",
                "status": "completed",
                "confidence_score": 0.7,
                "mock_data": True,
                "note": f"Generic mock result for agent type: {agent_id}"
            }

    
    async def _synthesize_results_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
        """Synthesize results from all agents using real synthesis agent"""
        logger.info(f"?? Synthesizing results for workflow {state['workflow_id']}")
        
        state["current_phase"] = WorkflowPhase.DATA_SYNTHESIS
        state["progress"] = 0.8
        
        # Send progress update
        await self._send_progress_update(state)
        
        try:
            # Use supervisor coordination for result synthesis (no separate synthesis agent)
            logger.info(f"ðŸ”„ Synthesizing results from {len(state['agent_results'])} agents using supervisor coordination")
            
            # Perform supervisor-based synthesis
            synthesis_result = await self._supervisor_synthesize_results(state)
            
            # Store synthesis results
            state["shared_context"]["synthesis_result"] = {
                "recommendations": synthesis_result.get("recommendations", []),
                "overall_score": synthesis_result.get("overall_score", 0.0),
                "confidence": synthesis_result.get("confidence_score", 0.8),
                "key_insights": synthesis_result.get("key_insights", []),
                "success_factors": synthesis_result.get("success_factors", []),
                "synthesized_at": datetime.now(UTC)
            }
            
            state["messages"].append(
                AIMessage(content=f"Results synthesized successfully. Overall score: {synthesis_result.get('overall_score', 0.0):.1f}")
            )
            
            logger.info(f"âœ… Synthesis completed with score: {synthesis_result.get('overall_score', 0.0):.1f}")
            
        except Exception as e:
            error_str = str(e).lower()
            
            # Check if this is a credential error
            if "credential" in error_str or "boto3" in error_str or "aws" in error_str:
                logger.warning(f" Synthesis failed due to missing credentials - using fallback synthesis")
            else:
                logger.error(f"??Result synthesis failed: {e}")
            
            # Fallback synthesis using simple aggregation
            try:
                successful_results = [r for r in state["agent_results"].values() if r.get("status") == "completed"]
                mock_results = [r for r in successful_results if r.get("mock_data", False)]
                
                avg_confidence = sum(r.get("confidence", 0.0) for r in successful_results) / max(len(successful_results), 1)
                
                # Generate comprehensive fallback synthesis
                recommendations = []
                key_insights = []
                success_factors = []
                
                if len(successful_results) > 0:
                    recommendations.append(f"Business validation completed with {len(successful_results)} agent analyses")
                    
                    if len(mock_results) > 0:
                        recommendations.append(f"Analysis includes {len(mock_results)} mock results due to missing API credentials")
                    
                    # Extract insights from agent results
                    for agent_id, result in state["agent_results"].items():
                        if result.get("status") == "completed":
                            agent_result = result.get("result", {})
                            if isinstance(agent_result, dict):
                                if agent_id == "market_analysis":
                                    market_size = agent_result.get("market_size", {}).get("current_usd", 0)
                                    if market_size > 0:
                                        key_insights.append(f"Market size estimated at ${market_size/1e9:.1f}B")
                                elif agent_id == "risk_assessment":
                                    risk_score = agent_result.get("overall_risk_score", 0)
                                    key_insights.append(f"Overall risk score: {risk_score:.2f}")
                                elif agent_id == "regulatory_compliance":
                                    compliance_score = agent_result.get("compliance_score", 0)
                                    key_insights.append(f"Compliance score: {compliance_score:.2f}")
                                elif agent_id == "fraud_detection":
                                    fraud_prevention = agent_result.get("fraud_prevention_value", 0)
                                    if fraud_prevention > 0:
                                        key_insights.append(f"Fraud prevention value: ${fraud_prevention/1e6:.1f}M")
                    
                    success_factors = [
                        "Multi-agent analysis framework",
                        "Automated business validation",
                        "Comprehensive market assessment"
                    ]
                    
                    if len(mock_results) == 0:
                        success_factors.append("Real-time data integration")
                    else:
                        success_factors.append("Resilient fallback processing")
                
                overall_score = min(avg_confidence * 100, 85.0)  # Cap at 85 for fallback
                
                state["shared_context"]["synthesis_result"] = {
                    "recommendations": recommendations,
                    "overall_score": overall_score,
                    "confidence": avg_confidence,
                    "key_insights": key_insights,
                    "success_factors": success_factors,
                    "synthesized_at": datetime.now(UTC),
                    "fallback": True,
                    "mock_data_count": len(mock_results),
                    "total_agents": len(state["agent_results"]),
                    "successful_agents": len(successful_results),
                    "original_error": str(e) if "credential" not in error_str else "Missing AWS credentials"
                }
                
                logger.info(f"??Fallback synthesis completed: {len(successful_results)} agents, score {overall_score:.1f}")
                
            except Exception as fallback_error:
                logger.error(f"??Fallback synthesis also failed: {fallback_error}")
                state["errors"].append({
                    "error": f"Synthesis failed: {e}, Fallback failed: {fallback_error}",
                    "timestamp": datetime.now(timezone.utc),
                    "phase": "data_synthesis"
                })
        
        state["last_updated"] = datetime.now(UTC)
        return state
    
    async def _quality_check_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
        """Perform quality check on workflow results"""
        logger.info(f"?? Quality check for workflow {state['workflow_id']}")
        
        state["current_phase"] = WorkflowPhase.QUALITY_ASSURANCE
        state["progress"] = 0.9
        
        try:
            # Calculate quality score based on agent results
            total_confidence = 0.0
            completed_agents = 0
            
            for agent_id, result in state["agent_results"].items():
                if result.get("status") == "completed":
                    total_confidence += result.get("confidence", 0.0)
                    completed_agents += 1
            
            if completed_agents > 0:
                average_confidence = total_confidence / completed_agents
                state["shared_context"]["quality_score"] = average_confidence
                
                # Check if quality meets threshold
                quality_passed = average_confidence >= self.config.quality_threshold
                state["shared_context"]["quality_passed"] = quality_passed
                
                logger.info(f"Quality score: {average_confidence:.2f}, Passed: {quality_passed}")
            else:
                state["shared_context"]["quality_score"] = 0.0
                state["shared_context"]["quality_passed"] = False
            
            state["messages"].append(
                AIMessage(content=f"Quality check completed. Score: {state['shared_context']['quality_score']:.2f}")
            )
            
        except Exception as e:
            logger.error(f"??Quality check failed: {e}")
            state["errors"].append({
                "error": str(e),
                "timestamp": datetime.now(timezone.utc),
                "phase": "quality_assurance"
            })
        
        state["last_updated"] = datetime.now(UTC)
        return state
    
    async def _finalize_workflow_node(self, state: AgentWorkflowState) -> AgentWorkflowState:
        """Finalize workflow execution"""
        logger.info(f"?? Finalizing workflow {state['workflow_id']}")
        
        state["current_phase"] = WorkflowPhase.COMPLETION
        state["progress"] = 1.0
        state["last_updated"] = datetime.now(UTC)
        
        state["messages"].append(
            AIMessage(content="Workflow completed successfully")
        )
        
        # Send final progress update
        await self._send_progress_update(state)
        
        return state
    
    def _should_retry_workflow(self, state: AgentWorkflowState) -> str:
        """Determine if workflow should retry or complete"""
        quality_passed = state["shared_context"].get("quality_passed", False)
        error_count = len(state["errors"])
        
        # Check if errors are credential-related (don't retry for these)
        credential_errors = any("credential" in str(error).lower() or "boto3" in str(error).lower() 
                               for error in state["errors"])
        
        # Check if we have mock data results (indicates credential issues handled)
        mock_data_results = any(result.get("mock_data", False) for result in state["agent_results"].values())
        
        # Accept lower quality scores when using mock data
        quality_threshold = 0.6 if mock_data_results else self.config.quality_threshold
        quality_acceptable = state["shared_context"].get("quality_score", 0.0) >= quality_threshold
        
        if quality_passed or quality_acceptable or credential_errors or mock_data_results:
            logger.info(f"Workflow {state['workflow_id']} completing (quality_passed={quality_passed}, quality_acceptable={quality_acceptable}, credential_errors={credential_errors}, mock_data={mock_data_results})")
            return "complete"
        elif error_count < self.config.max_retries:
            logger.info(f"Workflow {state['workflow_id']} will retry due to quality/errors")
            return "retry"
        else:
            logger.warning(f"Workflow {state['workflow_id']} exceeded max retries")
            return "complete"
    
    async def send_message(
        self,
        sender_id: str,
        recipient_id: str,
        message_type: MessageType,
        content: Dict[str, Any],
        priority: Priority = Priority.MEDIUM
    ) -> bool:
        """
        Send message between agents using AgentCore message routing.
        
        Args:
            sender_id: Sender agent ID
            recipient_id: Recipient agent ID
            message_type: Type of message
            content: Message content
            priority: Message priority
            
        Returns:
            bool: True if message sent successfully
        """
        try:
            message = AgentMessage(
                sender_id=sender_id,
                recipient_id=recipient_id,
                message_type=message_type,
                content=content,
                priority=priority
            )
            
            response = await self.agentcore_client.route_message(
                sender_id=sender_id,
                recipient_id=recipient_id,
                message=message
            )
            
            if response.success:
                logger.info(f"??Message routed from {sender_id} to {recipient_id}")
                return True
            else:
                logger.error(f"??Message routing failed: {response.error_message}")
                return False
            
        except Exception as e:
            logger.error(f"??Failed to send message: {e}")
            return False
    
    def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:
        """
        Get current status of a workflow.
        
        Args:
            workflow_id: Workflow identifier
            
        Returns:
            Dict containing workflow status or None if not found
        """
        if workflow_id in self.active_workflows:
            state = self.active_workflows[workflow_id]
            return {
                "workflow_id": workflow_id,
                "current_phase": state["current_phase"],
                "progress": state["progress"],
                "started_at": state["started_at"],
                "last_updated": state["last_updated"],
                "agent_count": len(state["agent_assignments"]),
                "error_count": len(state["errors"]),
                "quality_score": state["shared_context"].get("quality_score")
            }
        return None
    
    def get_active_workflows(self) -> List[str]:
        """
        Get list of active workflow IDs.
        
        Returns:
            List of active workflow IDs
        """
        return list(self.active_workflows.keys())
    
    async def _send_progress_update(self, state: AgentWorkflowState) -> None:
        """
        Send progress update via WebSocket.
        
        Args:
            state: Current workflow state
        """
        try:
            # Import here to avoid circular imports
            from riskintel360.api.websockets import send_validation_progress_update
            
            progress_data = {
                "status": "running" if state["progress"] < 1.0 else "completed",
                "progress_percentage": state["progress"] * 100,
                "current_phase": state["current_phase"].value,
                "message": f"Phase: {state['current_phase'].value.replace('_', ' ').title()}",
                "agent_results": state.get("agent_results", {}),
                "error_count": len(state.get("errors", []))
            }
            
            await send_validation_progress_update(state["workflow_id"], progress_data)
            
        except Exception as e:
            logger.error(f"Failed to send progress update: {e}")


@dataclass
class MarketCondition:
    """Market condition data structure"""
    condition_id: str
    condition_type: str  # 'market_shift', 'regulatory_change', 'competitive_move', etc.
    description: str
    impact_level: str  # 'low', 'medium', 'high', 'critical'
    affected_sectors: List[str]
    detected_at: datetime
    source: str
    confidence: float


@dataclass
class AlertEvent:
    """Alert event data structure"""
    alert_id: str
    event_type: str
    title: str
    description: str
    severity: str  # 'info', 'warning', 'error', 'critical'
    affected_workflows: List[str]
    created_at: datetime
    resolved_at: Optional[datetime] = None


class WorkflowOrchestrator:
    """
    Enhanced workflow orchestrator with real-time monitoring and alerting capabilities.
    Provides market condition monitoring, proactive alerting, and validation assumption tracking.
    """
    
    def __init__(
        self,
        supervisor_agent: Optional[SupervisorAgent] = None,
        monitoring_interval: int = 300  # 5 minutes
    ):
        """
        Initialize workflow orchestrator with monitoring capabilities.
        
        Args:
            supervisor_agent: Optional supervisor agent for workflow management (created if not provided)
            monitoring_interval: Monitoring interval in seconds
        """
        if supervisor_agent is None:
            # Try to create default supervisor agent, but handle failures gracefully
            try:
                from .agentcore_client import create_agentcore_client
                from .bedrock_client import create_bedrock_client
                
                agentcore_client = create_agentcore_client()
                bedrock_client = create_bedrock_client()
                supervisor_agent = SupervisorAgent(agentcore_client, bedrock_client)
                logger.info("??Created default supervisor agent with real clients")
                
            except Exception as e:
                logger.warning(f" Failed to create real supervisor agent: {e}")
                
                # Create a mock supervisor for testing/demo environments
                try:
                    from unittest.mock import Mock, AsyncMock
                    
                    # Create mock clients
                    mock_agentcore = Mock()
                    mock_agentcore.register_agent = Mock(return_value=True)
                    mock_agentcore.orchestrate_workflow = AsyncMock(return_value=Mock(success=True))
                    mock_agentcore.distribute_task = AsyncMock(return_value=Mock(success=True))
                    mock_agentcore.route_message = AsyncMock(return_value=Mock(success=True))
                    
                    mock_bedrock = Mock()
                    mock_bedrock.invoke_for_agent = Mock(return_value=Mock(content="Mock agent response"))
                    
                    supervisor_agent = SupervisorAgent(mock_agentcore, mock_bedrock)
                    logger.info("??Created mock supervisor agent for testing")
                    
                except Exception as mock_error:
                    logger.error(f"??Failed to create mock supervisor agent: {mock_error}")
                    supervisor_agent = None
        
        self.supervisor = supervisor_agent
        self.monitoring_interval = monitoring_interval
        
        # Monitoring state
        self.is_monitoring = False
        self.monitoring_task: Optional[asyncio.Task] = None
        
        # Market conditions and alerts
        self.market_conditions: Dict[str, MarketCondition] = {}
        self.active_alerts: Dict[str, AlertEvent] = {}
        self.alert_handlers: List[Callable[[AlertEvent], None]] = []
        
        # Validation assumptions tracking
        self.validation_assumptions: Dict[str, Dict[str, Any]] = {}
        self.assumption_change_handlers: List[Callable[[str, Dict[str, Any]], None]] = []
        
        logger.info("??WorkflowOrchestrator initialized with real-time monitoring")
    
    async def start_workflow(
        self,
        user_id: str,
        validation_request: Dict[str, Any],
        workflow_id: Optional[str] = None
    ) -> str:
        """
        Start a new validation workflow using the supervisor agent.
        
        Args:
            user_id: User initiating the workflow
            validation_request: Validation request data
            workflow_id: Optional workflow ID
            
        Returns:
            str: Workflow ID
        """
        if self.supervisor is None:
            raise RuntimeError("No supervisor agent available")
        
        return await self.supervisor.start_workflow(
            user_id=user_id,
            validation_request=validation_request,
            workflow_id=workflow_id
        )
    
    def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:
        """
        Get current status of a workflow.
        
        Args:
            workflow_id: Workflow identifier
            
        Returns:
            Dict containing workflow status or None if not found
        """
        if self.supervisor is None:
            return None
        
        return self.supervisor.get_workflow_status(workflow_id)
    
    def get_active_workflows(self) -> List[str]:
        """
        Get list of active workflow IDs.
        
        Returns:
            List of active workflow IDs
        """
        if self.supervisor is None:
            return []
        
        return self.supervisor.get_active_workflows()
    
    async def start_monitoring(self) -> None:
        """Start real-time monitoring system"""
        if self.is_monitoring:
            logger.warning(" Monitoring is already running")
            return
        
        self.is_monitoring = True
        self.monitoring_task = asyncio.create_task(self._monitoring_loop())
        logger.info("?? Real-time monitoring started")
    
    async def stop_monitoring(self) -> None:
        """Stop real-time monitoring system"""
        if not self.is_monitoring:
            return
        
        self.is_monitoring = False
        if self.monitoring_task:
            self.monitoring_task.cancel()
            try:
                await self.monitoring_task
            except asyncio.CancelledError:
                pass
        
        logger.info("?? Real-time monitoring stopped")
    
    async def _monitoring_loop(self) -> None:
        """Main monitoring loop for market conditions and system health"""
        logger.info("?? Starting monitoring loop")
        
        while self.is_monitoring:
            try:
                # Check market conditions
                await self._check_market_conditions()
                
                # Check system health
                await self._check_system_health()
                
                # Check validation assumptions
                await self._check_validation_assumptions()
                
                # Process alerts
                await self._process_alerts()
                
                # Wait for next monitoring cycle
                await asyncio.sleep(self.monitoring_interval)
                
            except asyncio.CancelledError:
                logger.info("?? Monitoring loop cancelled")
                break
            except Exception as e:
                logger.error(f"??Monitoring loop error: {e}")
                await asyncio.sleep(30)  # Wait before retrying
    
    async def _check_market_conditions(self) -> None:
        """Check for market condition changes"""
        try:
            # Simulate market condition detection
            # In a real implementation, this would integrate with market data APIs
            current_time = datetime.now(UTC)
            
            # Example: Detect significant market shifts
            market_conditions = [
                {
                    "condition_type": "market_shift",
                    "description": "Enterprise SaaS market showing 20% growth acceleration",
                    "impact_level": "high",
                    "affected_sectors": ["enterprise_software", "business_intelligence"],
                    "source": "market_data_api",
                    "confidence": 0.85
                },
                {
                    "condition_type": "regulatory_change",
                    "description": "New data privacy regulations affecting AI platforms",
                    "impact_level": "medium",
                    "affected_sectors": ["ai_platforms", "data_analytics"],
                    "source": "regulatory_monitor",
                    "confidence": 0.92
                }
            ]
            
            for condition_data in market_conditions:
                condition_id = f"{condition_data['condition_type']}_{current_time.timestamp()}"
                
                if condition_id not in self.market_conditions:
                    condition = MarketCondition(
                        condition_id=condition_id,
                        condition_type=condition_data["condition_type"],
                        description=condition_data["description"],
                        impact_level=condition_data["impact_level"],
                        affected_sectors=condition_data["affected_sectors"],
                        detected_at=current_time,
                        source=condition_data["source"],
                        confidence=condition_data["confidence"]
                    )
                    
                    self.market_conditions[condition_id] = condition
                    
                    # Create alert for significant market conditions
                    if condition.impact_level in ["high", "critical"]:
                        await self._create_alert(
                            event_type="market_condition_change",
                            title=f"Market Condition Alert: {condition.condition_type}",
                            description=condition.description,
                            severity="warning" if condition.impact_level == "high" else "critical",
                            affected_workflows=self._get_affected_workflows(condition.affected_sectors)
                        )
                    
                    logger.info(f"?? Market condition detected: {condition.description}")
        
        except Exception as e:
            logger.error(f"??Market condition check failed: {e}")
    
    async def _check_system_health(self) -> None:
        """Check system health and performance"""
        try:
            # Check active workflows
            active_workflows = self.supervisor.get_active_workflows()
            
            for workflow_id in active_workflows:
                status = self.supervisor.get_workflow_status(workflow_id)
                if status:
                    # Check for stuck workflows
                    time_since_update = datetime.now(UTC) - status["last_updated"]
                    if time_since_update > timedelta(minutes=30):
                        await self._create_alert(
                            event_type="workflow_stuck",
                            title=f"Workflow Stuck: {workflow_id}",
                            description=f"Workflow has not updated for {time_since_update}",
                            severity="warning",
                            affected_workflows=[workflow_id]
                        )
                    
                    # Check for high error rates
                    if status["error_count"] > 5:
                        await self._create_alert(
                            event_type="high_error_rate",
                            title=f"High Error Rate: {workflow_id}",
                            description=f"Workflow has {status['error_count']} errors",
                            severity="error",
                            affected_workflows=[workflow_id]
                        )
        
        except Exception as e:
            logger.error(f"??System health check failed: {e}")
    
    async def _check_validation_assumptions(self) -> None:
        """Check for changes in validation assumptions"""
        try:
            # Check each tracked validation for assumption changes
            for validation_id, assumptions in self.validation_assumptions.items():
                # Simulate assumption change detection
                # In a real implementation, this would compare current data with baseline assumptions
                
                current_assumptions = await self._get_current_assumptions(validation_id)
                
                changes_detected = False
                assumption_changes = {}
                
                for key, baseline_value in assumptions.items():
                    current_value = current_assumptions.get(key)
                    if current_value and self._is_significant_change(baseline_value, current_value):
                        assumption_changes[key] = {
                            "baseline": baseline_value,
                            "current": current_value,
                            "change_percentage": self._calculate_change_percentage(baseline_value, current_value)
                        }
                        changes_detected = True
                
                if changes_detected:
                    # Notify assumption change handlers
                    for handler in self.assumption_change_handlers:
                        try:
                            handler(validation_id, assumption_changes)
                        except Exception as handler_error:
                            logger.error(f"??Assumption change handler failed: {handler_error}")
                    
                    # Create alert for significant assumption changes
                    await self._create_alert(
                        event_type="validation_assumption_change",
                        title=f"Validation Assumptions Changed: {validation_id}",
                        description=f"Detected changes in {len(assumption_changes)} assumptions",
                        severity="info",
                        affected_workflows=[validation_id]
                    )
                    
                    logger.info(f"?? Assumption changes detected for validation {validation_id}")
        
        except Exception as e:
            logger.error(f"??Validation assumption check failed: {e}")
    
    async def _get_current_assumptions(self, validation_id: str) -> Dict[str, Any]:
        """Get current assumptions for a validation (simulated)"""
        # Simulate current market assumptions
        return {
            "market_size": 50000000000,  # $50B
            "growth_rate": 0.18,  # 18%
            "competition_intensity": 0.7,  # 70%
            "regulatory_risk": 0.3,  # 30%
            "customer_adoption_rate": 0.25  # 25%
        }
    
    def _is_significant_change(self, baseline: Any, current: Any) -> bool:
        """Check if change is significant enough to trigger alert"""
        if isinstance(baseline, (int, float)) and isinstance(current, (int, float)):
            change_percentage = abs((current - baseline) / baseline) if baseline != 0 else 0
            return change_percentage > 0.1  # 10% threshold
        return baseline != current
    
    def _calculate_change_percentage(self, baseline: Any, current: Any) -> float:
        """Calculate percentage change between baseline and current values"""
        if isinstance(baseline, (int, float)) and isinstance(current, (int, float)):
            if baseline != 0:
                return ((current - baseline) / baseline) * 100
        return 0.0
    
    async def _process_alerts(self) -> None:
        """Process and handle active alerts"""
        try:
            current_time = datetime.now(UTC)
            
            # Auto-resolve old alerts
            alerts_to_resolve = []
            for alert_id, alert in self.active_alerts.items():
                if alert.resolved_at is None:
                    # Auto-resolve info alerts after 1 hour
                    if alert.severity == "info" and (current_time - alert.created_at) > timedelta(hours=1):
                        alerts_to_resolve.append(alert_id)
                    # Auto-resolve warning alerts after 4 hours
                    elif alert.severity == "warning" and (current_time - alert.created_at) > timedelta(hours=4):
                        alerts_to_resolve.append(alert_id)
            
            for alert_id in alerts_to_resolve:
                await self._resolve_alert(alert_id)
        
        except Exception as e:
            logger.error(f"??Alert processing failed: {e}")
    
    async def _create_alert(
        self,
        event_type: str,
        title: str,
        description: str,
        severity: str,
        affected_workflows: List[str]
    ) -> str:
        """Create a new alert event"""
        alert_id = str(uuid.uuid4())
        
        alert = AlertEvent(
            alert_id=alert_id,
            event_type=event_type,
            title=title,
            description=description,
            severity=severity,
            affected_workflows=affected_workflows,
            created_at=datetime.now(UTC)
        )
        
        self.active_alerts[alert_id] = alert
        
        # Notify alert handlers
        for handler in self.alert_handlers:
            try:
                handler(alert)
            except Exception as handler_error:
                logger.error(f"??Alert handler failed: {handler_error}")
        
        logger.info(f"?Â¨ Alert created: {title} (Severity: {severity})")
        return alert_id
    
    async def _resolve_alert(self, alert_id: str) -> bool:
        """Resolve an active alert"""
        if alert_id in self.active_alerts:
            self.active_alerts[alert_id].resolved_at = datetime.now(UTC)
            logger.info(f"??Alert resolved: {alert_id}")
            return True
        return False
    
    def _get_affected_workflows(self, affected_sectors: List[str]) -> List[str]:
        """Get workflows affected by market condition changes"""
        # In a real implementation, this would match sectors to active workflows
        active_workflows = self.supervisor.get_active_workflows()
        return active_workflows[:2]  # Return first 2 as example
    
    def add_alert_handler(self, handler: Callable[[AlertEvent], None]) -> None:
        """Add an alert handler function"""
        self.alert_handlers.append(handler)
        logger.info("??Alert handler added")
    
    def add_assumption_change_handler(self, handler: Callable[[str, Dict[str, Any]], None]) -> None:
        """Add an assumption change handler function"""
        self.assumption_change_handlers.append(handler)
        logger.info("??Assumption change handler added")
    
    def track_validation_assumptions(self, validation_id: str, assumptions: Dict[str, Any]) -> None:
        """Track validation assumptions for change detection"""
        self.validation_assumptions[validation_id] = assumptions
        logger.info(f"?? Tracking assumptions for validation: {validation_id}")
    
    def get_market_conditions(self) -> List[MarketCondition]:
        """Get current market conditions"""
        return list(self.market_conditions.values())
    
    def get_active_alerts(self) -> List[AlertEvent]:
        """Get active alerts"""
        return [alert for alert in self.active_alerts.values() if alert.resolved_at is None]
    
    def get_monitoring_stats(self) -> Dict[str, Any]:
        """Get monitoring system statistics"""
        return {
            "is_monitoring": self.is_monitoring,
            "monitoring_interval": self.monitoring_interval,
            "market_conditions_count": len(self.market_conditions),
            "active_alerts_count": len(self.get_active_alerts()),
            "total_alerts_count": len(self.active_alerts),
            "tracked_validations_count": len(self.validation_assumptions),
            "alert_handlers_count": len(self.alert_handlers),
            "assumption_handlers_count": len(self.assumption_change_handlers)
        }


# Convenience function for creating workflow orchestrator
def create_workflow_orchestrator(
    agentcore_client: AgentCoreClient,
    bedrock_client: BedrockClient,
    config: Optional[WorkflowConfig] = None
) -> SupervisorAgent:
    """
    Create a new workflow orchestrator instance.
    
    Args:
        agentcore_client: AgentCore client for coordination
        bedrock_client: Bedrock client for LLM interactions
        config: Optional workflow configuration
        
    Returns:
        SupervisorAgent: Configured workflow orchestrator
    """
    return SupervisorAgent(agentcore_client, bedrock_client, config)


def create_enhanced_workflow_orchestrator(
    agentcore_client: AgentCoreClient,
    bedrock_client: BedrockClient,
    config: Optional[WorkflowConfig] = None,
    monitoring_interval: int = 300
) -> WorkflowOrchestrator:
    """
    Create a new enhanced workflow orchestrator with monitoring capabilities.
    
    Args:
        agentcore_client: AgentCore client for coordination
        bedrock_client: Bedrock client for LLM interactions
        config: Optional workflow configuration
        monitoring_interval: Monitoring interval in seconds
        
    Returns:
        WorkflowOrchestrator: Enhanced workflow orchestrator with monitoring
    """
    supervisor = SupervisorAgent(agentcore_client, bedrock_client, config)
    return WorkflowOrchestrator(supervisor, monitoring_interval)
